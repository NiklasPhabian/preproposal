Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Rilee2016,
abstract = {We have implemented an updated Hierarchical Triangular Mesh (HTM) as the basis for a unified data model and an indexing scheme for geoscience data to address the variety challenge of Big Earth Data. In the absence of variety, the volume challenge of Big Data is relatively easily addressable with parallel processing. The more important challenge in achieving optimal value with a Big Data solution for Earth Science (ES) data analysis, however, is being able to achieve good scalability with variety. With HTM unifying at least the three popular data models, i.e. Grid, Swath, and Point, used by current ES data products, data preparation time for integrative analysis of diverse datasets can be drastically reduced and better variety scaling can be achieved. HTM is also an indexing scheme, and when applied to all ES datasets, data placement alignment (or co-location) on the shared nothing architecture, which most Big Data systems are based on, is guaranteed and better performance is ensured. With HTM most geospatial set operations become integer interval operations with further performance advantages},
annote = {The authors address the limitations associated with the file-centric data accesss in which users need to go through ETL processes prior to interacting with the data. In particular, they recognize the heterogeneity of data and their misalignment as a major bottleneck in scientific computing. The authors propose a system capeable of in-place analysis liberating researchers of the need to go through ETL processes prior to analysis. The key technology for this system is indexing of geospatial data through HTM to integrate diverse data.},
author = {Rilee, Michael Lee and Kuo, Kwo-Sen and Clune, Thomas and Oloso, Amidu and Brown, Paul and Yu, Hongfeng},
booktitle = {2016 IEEE International Conference on Big Data, Big Data 2016},
doi = {https://doi.org/10.1109/BigData.2016.7840700},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rilee et al. - 2016 - Addressing the big-earth-data variety challenge with the hierarchical triangular mesh.pdf:pdf;:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rilee et al. - 2016 - Addressing the big-earth-data variety challenge with the hierarchical triangular mesh(2).pdf:pdf},
isbn = {9781467390040},
keywords = {DAAC,GIS,HTM,SciDB,array database,data analysis,data fusion,geographic metadata,global,global index,indexing,load balancing,remote sensing,shared nothing architecture,variety},
title = {{Addressing the big-earth-data variety challenge with the hierarchical triangular mesh}},
year = {2016}
}
@article{Appel2018,
abstract = {Earth observation (EO) datasets are commonly provided as collection of scenes, where individual scenes represent a temporal snapshot and cover a particular region on the Earth's surface. Using these data in complex spatiotemporal modeling becomes difficult as soon as data volumes exceed a certain capacity or analyses include many scenes, which may spatially overlap and may have been recorded at different dates. In order to facilitate analytics on large EO datasets, we combine and extend the geospatial data abstraction library (GDAL) and the array-based data management and analytics system SciDB. We present an approach to automatically convert collections of scenes to multidimensional arrays and use SciDB to scale computationally intensive analytics. We evaluate the approach in three study cases on national scale land use change monitoring with Landsat imagery, global empirical orthogonal function analysis of daily precipitation, and combining historical climate model projections with satellite-based observations. Results indicate that the approach can be used to represent various EO datasets and that analyses in SciDB scale well with available computational resources. To simplify analyses of higher-dimensional datasets as from climate model output, however, a generalization of the GDAL data model might be needed. All parts of this work have been implemented as open-source software and we discuss how this may facilitate open and reproducible EO analyses.},
author = {Appel, Marius and Lahn, Florian and Buytaert, Wouter and Pebesma, Edzer},
doi = {10.1016/j.isprsjprs.2018.01.014},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Appel et al. - 2018 - Open and scalable analytics of large Earth observation datasets From scenes to multidimensional arrays using Sc(2).pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Big data,Database systems,Earth observation,Open science,Scalable analytics,Spatial information science,global,global index},
pages = {47--56},
publisher = {The Authors},
title = {{Open and scalable analytics of large Earth observation datasets: From scenes to multidimensional arrays using SciDB and GDAL}},
url = {https://doi.org/10.1016/j.isprsjprs.2018.01.014},
volume = {138},
year = {2018}
}
@inproceedings{Kuo2017,
abstract = {As a universal geoscience data representation, the Spatio- Temporal Adaptive-Resolution Encoding, STARE, is bring- ing about unprecedented interoperability to all Earth Science data. In its spatial component, STARE contracts the usual two-dimensional, i.e. latitude and longitude, geolocation into a one-dimensional, hierarchical index. The STARE geolocation index follows the quadfurcation scheme of the well-established hierarchical triangular mesh (HTM) used in astronomy but with an innovative bit-field arrangement that includes approximate data resolution information to enable efficient geospatial set operations. STARE's temporal com- ponent is also hierarchical with bit fields referring to con- ventional date-time intervals or units. STARE is designed for geo-spatiotemporal data placement alignment in data- bases (e.g. SciDB) but also supports more traditional con- texts via a STARE application programming interface (API).},
address = {Toulouse, France},
annote = {The Spatio Temoral Adaptive Resolution Encoding (STARE) is an extention of the Hierachical Triangular Mesh (HTM). STARE offers a solution to the variety challenge in geospatioal sciences by providing a univeral indexing method to spatiotemporal data. STARE is developped to support data placement alignment in array databases whilst encoding resolution information into the index.},
author = {Kuo, Kwo-Sen and Rilee, Michael Lee},
booktitle = {2017 Conference on Big Data from Space},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Rilee - Unknown - STARE -TOWARD UNPRECEDENTED GEO-DATA INTEROPERABILITY(2).pdf:pdf},
keywords = {global,global index},
number = {November},
title = {{STARE - Toward Unprecedented Geo-Data interoperability}},
url = {https://www.researchgate.net/publication/320908197{\_}STARE{\_}-{\_}TOWARD{\_}UNPRECEDENTED{\_}GEO-DATA{\_}INTEROPERABILITY},
year = {2017}
}
@article{TheSciDBDevelopmentTeam2010,
abstract = {SciDB [4, 3] is a new open-source data management system intended primarily for use in application domains that involve very large (petabyte) scale array data; for example, scientific applications such as astronomy, remote sensing and climate modeling, bio-science information management, as well as commercial applications such as risk management systems in the financial services sector, and the analysis of web log data. In this talk we will describe our set of motivating examples and use them to explain the features of SciDB. We then provide a snapshot of the project â€˜in flight', describing our novel storage manager, array data model, query language, and extensibility frameworks.},
author = {{The SciDB Development Team}},
doi = {10.1145/1807167.1807271},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The SciDB Development Team - 2010 - Overview of SciDB - Large Scale Array Storage , Processing and Analysis.pdf:pdf},
isbn = {9781450300322},
issn = {07308078},
journal = {ACM SIGMOD International Conference on Management of Data},
keywords = {acm sigmod industrial proceedings,scientific data man-},
pages = {963--968},
title = {{Overview of SciDB - Large Scale Array Storage , Processing and Analysis}},
url = {http://www.scidb.org},
year = {2010}
}
@article{Kim2018,
abstract = {{\textcopyright} 2017 IEEE. SciDB is an array-based DBMS popularly used for scientific data analysis. One major hurdle in processing large-scale scientific data is pre-processing before loading data, which includes extraneous file conversion from a raw data format to a software-specific format, which causes a significant I/O overhead. Moreover, data loading is typically followed by array transformation, which requires data to be sorted and redistributed by hashing. In order to reduce the overhead, we streamline the conversion process and modify the distribution method in loading stages. In addition, we eliminate two heavy-duty steps, namely sort and redistribution, which account for a dominant portion of the redimensioning cost. Our experiments show that the data loading time can be reduced up to 65{\%} compared with the vanilla loader of SciDB.},
author = {Kim, Sangchul and Lee, Junhee and Kim, Taehoon and Moon, Bongki},
doi = {10.1109/BigData.2017.8258331},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2018 - Scalable parallel data loading in SciDB.pdf:pdf},
isbn = {9781538627143},
journal = {Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017},
keywords = {Array Database Model,Data Loading,SciDB},
pages = {3443--3446},
title = {{Scalable parallel data loading in SciDB}},
volume = {2018-Janua},
year = {2018}
}
@phdthesis{Planthaber2012b,
abstract = {MODBASE, a collection of tools and practices built around the open source SciDB multidimensional data management and analytics software system, provides the Earth Science community with a powerful foundation for direct, ad-hoc analysis of large volumes of Level-1B data produced by the NASA Moderate Resolution Imaging Spectroradiometer (MODIS) instrument. This paper details the reasons for building the MODBASE system, its design and implementation, and puts it to the test on a series of practical Earth Science benchmarks using standard MODIS data granules.},
author = {Planthaber, Gary},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Planthaber - 2012 - MODBASE A SciDB-Powered System for Large-Scale Distributed Storage and Analysis of MODIS Earth Remote Sensing Dat(2).pdf:pdf},
keywords = {global,global index},
title = {{MODBASE: A SciDB-Powered System for Large-Scale Distributed Storage and Analysis of MODIS Earth Remote Sensing Data}},
year = {2012}
}
@article{Appel2018,
abstract = {Earth observation (EO) datasets are commonly provided as collection of scenes, where individual scenes represent a temporal snapshot and cover a particular region on the Earth's surface. Using these data in complex spa-tiotemporal modeling becomes difficult as soon as data volumes exceed a certain capacity or analyses include many scenes, which may spatially over-lap and may have been recorded at different dates. In order to facilitate analytics on large EO datasets, we combine and extend the geospatial data abstraction library (GDAL) and the array-based data management and an-alytics system SciDB. We present an approach to automatically convert col-lections of scenes to multidimensional arrays and use SciDB to scale compu-tationally intensive analytics. We evaluate the approach in three study cases on national scale land use change monitoring with Landsat imagery, global empirical orthogonal function analysis of daily precipitation, and combining historical climate model projections with satellite-based observations. Re-sults indicate that the approach can be used to represent various EO datasets and that analyses in SciDB scale well with available computational resources. To simplify analyses of higher-dimensional datasets as from climate model output, however, a generalization of the GDAL data model might be needed. All parts of this work have been implemented as open-source software and we discuss how this may facilitate open and reproducible EO analyses.},
author = {Appel, Marius and Lahn, Florian and Buytaert, Wouter and Pebesma, Edzer},
doi = {10.1016/j.isprsjprs.2018.01.014},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Appel et al. - 2018 - Open and scalable analytics of large Earth observation datasets From scenes to multidimensional arrays using SciDB.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Database systems,Earth observation,Open,Scalable analytics},
month = {apr},
pages = {47--56},
title = {{Open and scalable analytics of large Earth observation datasets: From scenes to multidimensional arrays using SciDB and GDAL}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0924271617300898},
volume = {138},
year = {2018}
}
@incollection{Brunner2001,
abstract = {Astronomy has a long history of acquiring, systematizing, and interpreting large quantities of data. Starting from the earliest sky atlases through the first major photographic sky surveys of the 20th century, this tradition is continuing today, and at an ever increasing rate. Like many other fields, astronomy has become a very data-rich science, driven by the advances in telescope, detector, and computer technology. Numerous large digital sky surveys and archives already exist, with information content measured in multiple Terabytes, and even larger, multi-Petabyte data sets are on the horizon. Systematic observations of the sky, over a range of wavelengths, are becoming the primary source of astronomical data. Numerical simulations are also producing comparable volumes of information. Data mining promises to both make the scientific utilization of these data sets more effective and more complete, and to open completely new avenues of astronomical research. Technological problems range from the issues of database design and federation, to data mining and advanced visualization, leading to a new toolkit for astronomical research. This is similar to challenges encountered in other data-intensive fields today. These advances are now being organized through a concept of the Virtual Observatories, federations of data archives and services representing a new information infrastructure for astronomy of the 21st century. In this article, we provide an overview of some of the major datasets in astronomy, discuss different techniques used for archiving data, and conclude with a discussion of the future of massive datasets in astronomy.},
archivePrefix = {arXiv},
arxivId = {astro-ph/0106481},
author = {Brunner, Robert J. and Djorgovski, S. George and Prince, Thomas A. and Szalay, Alex S.},
doi = {10.1007/978-1-4615-0005-6_27},
eprint = {0106481},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brunner et al. - 2002 - Massive Datasets in Astronomy.pdf:pdf},
isbn = {1-4020-0489-3},
keywords = {global,global index},
pages = {931--979},
primaryClass = {astro-ph},
title = {{Massive Datasets in Astronomy}},
url = {http://arxiv.org/abs/astro-ph/0106481 http://link.springer.com/10.1007/978-1-4615-0005-6{\_}27},
year = {2002}
}
@article{Gorelick2017,
abstract = {Google Earth Engine is a cloud-based platform for planetary-scale geospatial analysis that brings Google's massive computational capabilities to bear on a variety of high-impact societal issues including deforestation, drought, disaster, disease, food security, water management, climate monitoring and environmental protection. It is unique in the field as an integrated platform designed to empower not only traditional remote sensing scientists, but also a much wider audience that lacks the technical capacity needed to utilize traditional supercomputers or large-scale commodity cloud computing resources.},
author = {Gorelick, Noel and Hancher, Matt and Dixon, Mike and Ilyushchenko, Simon and Thau, David and Moore, Rebecca},
doi = {10.1016/j.rse.2017.06.031},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gorelick et al. - 2017 - Google Earth Engine Planetary-scale geospatial analysis for everyone.pdf:pdf},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Analysis,Big data,Cloud computing,Data democratization,Earth Engine,Platform},
month = {dec},
pages = {18--27},
publisher = {The Author(s)},
title = {{Google Earth Engine: Planetary-scale geospatial analysis for everyone}},
url = {https://doi.org/10.1016/j.rse.2017.06.031 https://linkinghub.elsevier.com/retrieve/pii/S0034425717302900},
volume = {202},
year = {2017}
}
@inproceedings{Doan2016,
abstract = {We investigate the impact of data placement on two Big Data technologies, Spark and SciDB, with a use case from Earth Science where data arrays are multidimensional. Simultaneously, this investigation provides an opportunity to evaluate the performance of the technologies involved. Two datastores, HDFS and Cassandra, are used with Spark for our comparison. It is found that Spark with Cassandra performs better than with HDFS, but SciDB performs better yet than Spark with either datastore. The investigation also underscores the value of having data aligned for the most common analysis scenarios in advance on a shared nothing architecture. Otherwise, repartitioning needs to be carried out on the fly, degrading overall performance.},
address = {Washington D.C.,USA},
author = {Doan, Khoa and Oloso, Amidu and Kuo, Kwo-Sen and Clune, Thomas and Yu, Hongfeng and Nelson, Brian and Zhang, Jian},
booktitle = {2016 IEEE International Conference on Big Data (Big Data)},
doi = {10.1109/BigData.2016.7840621},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doan et al. - 2016 - Evaluating the Impact of Data Placement to Spark and SciDB with an Earth Science Use Case.pdf:pdf},
keywords = {Arrays,Big Data,Big data,Cassandra,Data analysis,Earth science,Geoscience,HDFS,Indexing,Parallel processing,SciDB,Spark,Sparks,data layout,data placement,datastores,distributed databases,global,global index,multidimensional data arrays,multimensional arrays},
mendeley-tags = {Arrays,Big Data,Big data,Cassandra,Data analysis,Earth science,Geoscience,HDFS,Indexing,Parallel processing,SciDB,Spark,Sparks,data layout,data placement,datastores,distributed databases,multidimensional data arrays,multimensional arrays},
pages = {341--346},
title = {{Evaluating the Impact of Data Placement to Spark and SciDB with an Earth Science Use Case}},
url = {https://ieeexplore.ieee.org/document/7840621/},
year = {2016}
}
@article{Stonebraker2013,
author = {Stonebraker, Michael and Brown, Paul and Zhang, Donghui and Becla, Jacek},
doi = {10.1109/MCSE.2013.19},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stonebraker et al. - 2013 - SciDB A Database Management System for Applications with Complex Analytics.pdf:pdf},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
month = {may},
number = {3},
pages = {54--62},
title = {{SciDB: A Database Management System for Applications with Complex Analytics}},
url = {papers3://publication/uuid/CE7E41BF-97DA-43B9-A3CB-5FBE78C47A0D http://ieeexplore.ieee.org/document/6461866/},
volume = {15},
year = {2013}
}
@article{Xing2017,
abstract = {Scientists are increasingly turning to datacenter-scale com-puters to produce and analyze massive arrays. Despite decades of database research that extols the virtues of declarative query processing, scientists still write, debug and parallelize imperative HPC kernels even for the most mundane queries. This impedance mismatch has been partly attributed to the cumbersome data loading process; in response, the database community has proposed in situ mechanisms to access data in scientific file formats. Scientists, however, desire more than a passive access method that reads arrays from files. This paper describes ArrayBridge, a bi-directional array view mechanism for scientific file formats, that aims to make declarative array manipulations interoperable with impera-tive file-centric analyses. Our prototype implementation of ArrayBridge uses HDF5 as the underlying array storage li-brary and seamlessly integrates into the SciDB open-source array database system. In addition to fast querying over external array objects, ArrayBridge produces arrays in the HDF5 file format just as easily as it can read from it. Ar-rayBridge also supports time travel queries from impera-tive kernels through the unmodified HDF5 API, and auto-matically deduplicates between array versions for space ef-ficiency. Our extensive performance evaluation in NERSC, a large-scale scientific computing facility, shows that Array-Bridge exhibits statistically indistinguishable performance and I/O scalability to the native SciDB storage engine.},
archivePrefix = {arXiv},
arxivId = {arXiv:1702.08327v1},
author = {Xing, Haoyuan and Floratos, Sofoklis and Blanas, Spyros and Byna, Suren and Wu, Kesheng and Brown, Paul},
eprint = {arXiv:1702.08327v1},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xing et al. - 2017 - ArrayBridge Interweaving declarative array processing with high-performance computing.pdf:pdf},
journal = {CoRR},
keywords = {global,global index},
title = {{ArrayBridge: Interweaving declarative array processing with high-performance computing}},
url = {https://arxiv.org/abs/1702.08327},
volume = {abs/1702.0},
year = {2017}
}
@techreport{OpenGeospatialConsortium2017,
abstract = {This document specifies the core Abstract Specification and extension mechanisms for Discrete Global Grid Systems (DGGS). A DGGS is a spatial reference system that uses a hierarchical tessellation of cells to partition and address the globe. DGGS are characterized by the properties of their cell structure, geo-encoding, quantization strategy and associated mathematical functions. The OGC DGGS Abstract Specification supports the specification of standardized DGGS infrastructures that enable the integrated analysis of very large, multi-source, multi-resolution, multi-dimensional, distributed geospatial data. Interoperability between OGC DGGS implementations is anticipated through implementation standards, and extension interface encodings of OGC Web Services.},
author = {{Open Geospatial Consortium}},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Open Geospatial Consortium - 2017 - Topic 21 Discrete Global Grid Systems Abstract Specification.pdf:pdf},
institution = {Open Geospatial Consortium},
keywords = {global,global index},
title = {{Topic 21: Discrete Global Grid Systems Abstract Specification}},
url = {http://docs.opengeospatial.org/as/15-104r5/15-104r5.html},
year = {2017}
}
@misc{Rilee,
abstract = {Few research fields may claim a longer history than geoscience regarding the challenges of voluminous and diverse data. Today, the severity of these challenges is intensifying and the urgency to address them height-ening. While technology advancements in computation are leading to model simulations with higher spati-otemporal resolutions, similar advancements in instrumentation have increased observation resolutions and, in addition, greater observation diversity. The corresponding increase in data volume requires more storage to archive and more computer power to process and analyze. It is the increase in data variety, under the existing data practice, that requires disproportionately more labor and time to perform integrative, interdis-ciplinary analyses that are required for a complex system of systems like Earth. Computation speed has long sur-passed I/O capacity [1]. Data movement bandwidths are thus the crucial factors in gaining a fundamental understand of the challenges we are facing. We start with inspecting these bandwidths shown in Figure 1 [2] and Figure 2. Figure 1 depicts the trends (and projections) of bandwidths for memory, network, and storage from 1995 to 2025, whereas Figure 2 shows that of Internet user's bandwidth from 1988 to 2018. Note that in Figure 1, DRAM bandwidth is ex-pressed in gigabit per second (GB/s) and is used as a surrogate for compute bandwidth, whereas the other two (i.e. network and storage bandwidths) are in megabit per second (MB/s). Note also that the network bandwidth referred to here is the high-perfor-mance network interconnecting computer cluster nodes, e.g. Infiniband or fiber optic cable, which is usually shared among tens, hundreds, or more nodes. Their effective bandwidth per node is thus comparable or an order of magnitude, i.e. O(1), less than storage bandwidth. The storage and memory bandwidths, on the other hand, are for dedicated connections to a computer node or its CPUs. Let's consider the period of 1995-2015. Since 1995, due to the popularization of solid state drives (SSDs), the rate of increase for storage bandwidth has kept pace with Moore's Law [3], i.e. growing {\~{}}60{\%} per year, so has that for network bandwidth. Although the rate of increase for memory bandwidth at {\~{}}46{\%} a year has lagged behind Moore's Law, pro-jection still shows that it is likely to maintain the lead in magnitude over network bandwidth until {\~{}}2025. The more distressing fact is the relative trend of In-ternet user's bandwidth, which has followed Niel-son's Law [4] growing at {\~{}}50{\%} per year (Figure 2) and is O(1) or more slower than the others. If the trend continues the disparity between Internet user's bandwidth and the others will widen. In summary, there is a succession of data connections away from the CPU, from memory, storage, network, to Internet, with progressively smaller bandwidth at a difference of {\~{}}O(1) at each step. Moving data over Internet to processing is thus becoming less and less efficient. We therefore often hear the pro-verbial advice: We should move processing to data rather than data to processing [5]. A more concise Figure 1. Trends of memory, storage, and high-performance network bandwidths. DRAM BW in GB/s; Network BW and SSD BW in MB/s. Figure 2. Trend of Internet user's bandwidth (in bits/s). 2 version of the advice is: We need to exploit compute-storage affinity, which is now a generally understood principle. The key question naturally becomes: " How to best exploit compute-storage affinity for opti-mal effect and value? " , a question central to our previous investigations, which we summarize below. 1.1 Existing Data Practice and Its Predicaments We first examine the existing data practice and the predicaments associated with such a practice. The predicaments are underlined and italicized for better recognition. For decades, geoscience data practice has followed a two-step approach: 1) package data into files (aka granules) for archival and distribution and 2) catalog metadata of the datasets and granules into databases managed by relational database management systems (RDBMSs), making them discoverable and searcha-ble. The establishment of popular distributed active archive centers (DAACs) as data warehouses and the standardization of data file formats through HDF/netCDF [6, 7] Application Programming Interface (API) by NASA Earth Observing System Data Information System (EOSDIS) since the 1990s exemplify this approach, which has become a de facto standard followed by other domestic/international organizations. Despite its success, this 2-step approach has reached its limit with today's demands. Users cannot ma-nipulate the data directly but must work through their containers (i.e. files) requiring learned skills and resources beyond mere scientific analysis. It inevitably leads to data transfer, mostly via low-bandwidth Internet connections, for processing or integration by users. This alone decimates volume scaling. Lacking server-side " in-place " access, users must follow FTP links returned by metadata searches to download/transfer data files before starting analysis. To prepare, users (or their institutions) must first pro-cure compute and storage resources, including associated management and maintenance. Researchers must also engage in data management activities, such as file organization and backup, as well as familiarize themselves with file structures and semantics. Largely irrelevant to research, these tasks unnecessarily en-cumber researchers, hampering productivity. The most disheartening aspect is perhaps the collective waste, since almost every such effort needs to duplicate this process and the resources it requires, even when the same datasets are used and similar analyses are performed. Downloaded data become " local " and " individual, " subject to end users' preferences, e.g. data man-agement policies and programming language choices. The inevitable profusion of these preferences erects expansive barriers to collaboration. Moreover, most geoscience researchers are not professional software engineers, rarely following software engineering practices, e.g. unit testing and source code version control. Thus, software quality varies and reproducibility is compromised. The problem is further compounded by the enormous variety of geoscience data that arises from varying geometries and resolutions used for model simulations and, even more, from the different modes, resolu-tions, and geometries of observations. For integrating diverse datasets, each file in each variety must be processed and homogenized before integration. Unless the " fused " data of the integrative analysis is saved and shared, anyone who wishes to perform the same analysis must repeat the process. If a similar, but not identical, integrative analysis needs to be performed, e.g. using different spatiotemporal subsets or slightly different data products, similar data preparation often must be repeated; prior results and intermediate prod-ucts can seldom be reused. Data preparation costs exert an expensive toll at every such iteration. This leads to poor variety scaling. 1.2 Candidate Solution Approaches We categorize the solution approaches according to the strength of coupling between storage and memory, and thus compute. These approaches span a spectrum, from uncoupled, barely coupled, loosely coupled, to tightly coupled. The existing approach is at the uncoupled end of the spectrum, for which both storage and compute are " diverged " , meaning that data (files) hosted at data centers are reachable mostly via Internet whereas compute are local/individual efforts of the researchers. Data traffic is severely skewed towards the narrowest bandwidth, giving rise to present challenges. The next category of approaches up the spectrum, barely coupled, include those that move the data center holdings to either traditional high-performance computing (HPC) centers or Cloud data centers, without modifying the 2-step data practice. This approach at least has storage " converged " , with the higher network bandwidth within the HPC or Cloud center handling most of the data movements. Collaboration},
author = {Rilee, Michael Lee and Kuo, Kwo-Sen},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rilee, Kuo - Unknown - Project Description EarthCube Integration.pdf:pdf},
keywords = {global,global index},
title = {{Project Description EarthCube Integration}}
}
@inproceedings{Krcal2015,
abstract = {Current research in climate informatics focuses mainly on the development of novel (machine learning, data mining, or statistical) techniques to analyze climate data (e.g. model, in-situ, or satellite) or to make prediction based on these climate data. One important component missing from this analysis workflow is data management that allows efficient and flexible data retrieval, (ease of) reproducibility, and the (ease of) techniques reuse on user-defined data subsets or other data. In this paper, we describe our preliminary investigation on the utilization of the distributed array-based database management system, SciDB, to support data-driven climate science research. We focus on modeling and generating in-dices that allow effective execution of various spatiotemporal queries on satellite data. Moreover, we demonstrate fast and accurate data retrieval based on user-specified trajectories from the SciDB database containing tropical cyclone trajec-tories and the complete ten-year QuikSCAT ocean surface wind fields satellite data. Our preliminary work indicates the feasibility of the array-based technology for multiple satellite data storage, query, and analysis. Towards this end, a successful deployment of SciDB-based data storage can facilitate the use of data from multiple satellites for climate and weather research.},
address = {Bellevue, WA, USA},
author = {Kr{\v{c}}{\'{a}}l, Lubo{\v{s}} and Ho, Shen-Shyang},
booktitle = {BigSpatial'15 Proceedings of the 4th International ACM SIGSPATIAL Workshop on Analytics for Big Geospatial Data},
doi = {10.1145/2835185.2835190},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kr{\v{c}}{\'{a}}l, Ho - 2015 - A SciDB-based Framework for Efficient Satellite Data Storage and Query based on Dynamic Atmospheric Event Trajector.pdf:pdf},
keywords = {Database Managementâ€” Database Applications,Design,Measurement Keywords SciDB,QuikSCAT,SciDB,Scientific databases General Terms Experimentation,arrays,global,global index,index,indexing,multidimensional arrays,satellite data,scientific database,spatiotemporal},
mendeley-tags = {QuikSCAT,SciDB,arrays,index,indexing,multidimensional arrays,satellite data,scientific database,spatiotemporal},
pages = {7--14},
publisher = {ACM},
title = {{A SciDB-based Framework for Efficient Satellite Data Storage and Query based on Dynamic Atmospheric Event Trajectory}},
url = {http://doi.acm.org/10.1145/2835185.2835190 https://dl.acm.org/citation.cfm?id=2835190},
year = {2015}
}
@article{Wiscombe1980,
author = {Wiscombe, Warren J. and Warren, Stephen G.},
doi = {10.1175/1520-0469(1980)037<2712:AMFTSA>2.0.CO;2},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiscombe, Warren - 1980 - A Model for the Spectral Albedo of Snow. I Pure Snow.pdf:pdf},
issn = {0022-4928},
journal = {Journal of the Atmospheric Sciences},
month = {dec},
number = {12},
pages = {2712--2733},
title = {{A Model for the Spectral Albedo of Snow. I: Pure Snow}},
url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0469{\%}281980{\%}29037{\%}3C2712{\%}3AAMFTSA{\%}3E2.0.CO{\%}3B2},
volume = {37},
year = {1980}
}
@phdthesis{Hausen2016,
abstract = {Hyperspectral Imaging is a technique that collects information from the electromagnetic spectrum, storing the value of the spectrum band for each pixel of the image. This technique stands out for the contiguous wide range of wavelengths it covers; leading to the ability of accurate surface and material distinction. The big volumes of Hyperspectral Images datasets, which are called data cubes as the band value represent the third dimension, have been a barrier against exploiting the full potential of these images where there is no standardized way in storing them. On top of that, the classical relational databases proved to be an inconvenient storage space for such images. Array databases have been a serious choice for storing scientific and big volumes of data, and they represent a promising suitable environment for hyperspectral images. We aim to study the efficiency of storing hyperspectral images on an array-database by suggesting a convenient data model. Furthermore, in order to examine the feasibility of this model, we make a comparison with two relational databases using specific measurements in performance and query complexity.},
author = {Hausen, Eias},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hausen - 2016 - Array-database Model (SciDB) for Standardized Storing of Hyperspectral Satellite Images.pdf:pdf},
keywords = {Array Database,Database Comparison,GeoTIFF Images,Hyperspectral Imaging,Hyperspectral Satellite Images,Raster Database,Satellite Images Storage,SciDB,global,global index},
mendeley-tags = {Array Database,Database Comparison,GeoTIFF Images,Hyperspectral Imaging,Hyperspectral Satellite Images,Raster Database,Satellite Images Storage,SciDB},
school = {Erasmus-Mundus},
title = {{Array-database Model (SciDB) for Standardized Storing of Hyperspectral Satellite Images}},
type = {Master in Geospatial Technologies (Erasmus-Mundus)},
url = {http://hdl.handle.net/10362/18407},
year = {2016}
}
@misc{Rile2016,
author = {Rilee, Michael Lee and Clune, Thomas and Kuo, Kwo-Sen and Oloso, Amidu},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rilee et al. - 2016 - Scaling to Diversity The DERECHOS Distributed Infrastructure for Analyzing and Sharing Dat.pdf:pdf},
keywords = {global,global index},
title = {{Scaling to Diversity: The DERECHOS Distributed Infrastructure for Analyzing and Sharing Data}},
year = {2016}
}
@article{Tan2017,
abstract = {Over the past few years, Earth Observation (EO) has been continuously generating much spatiotemporal data that serves for societies in resource surveillance, environment protection, and disaster prediction. The proliferation of EO data poses great challenges in current approaches for data management and processing. Nowadays, the Array Database technologies show great promise in managing and processing EO Big Data. This paper suggests storing and processing EO data as multidimensional arrays based on state-of-the-art array database technologies. A multidimensional spatiotemporal array model is proposed for EO data with specific strategies for mapping spatial coordinates to dimensional coordinates in the model transformation. It allows consistent query semantics in databases and improves the in-database computing by adopting unified array models in databases for EO data. Our approach is implemented as an extension to SciDB, an open-source array database. The test shows that it gains much better performance in the computation compared with traditional databases. A forest fire simulation study case is presented to demonstrate how the approach facilitates the EO data management and in-database computation.},
author = {Tan, Zhenyu and Yue, Peng and Gong, Jianya},
doi = {10.3390/ijgi6070220},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan, Yue, Gong - 2017 - An Array Database Approach for Earth Observation Data Management and Processing.pdf:pdf},
issn = {2220-9964},
journal = {ISPRS International Journal of Geo-Information},
keywords = {array database,big data,earth observation,forest fire simulation,multidimensional array,scidb},
month = {jul},
number = {7},
pages = {220},
title = {{An Array Database Approach for Earth Observation Data Management and Processing}},
url = {http://www.mdpi.com/2220-9964/6/7/220},
volume = {6},
year = {2017}
}
@inproceedings{Kondor2014,
abstract = {We present a case study about the spatial indexing and regional clas-sification of billions of geographic coordinates from geo-tagged social net-work data using Hierarchical Triangular Mesh (HTM) implemented for Microsoft SQL Server. Due to the lack of certain features of the HTM library, we use it in conjunction with the GIS functions of SQL Server to significantly increase the efficiency of pre-filtering of spatial filter and join queries. For example, we implemented a new algorithm to compute the HTM tessellation of complex geographic regions and precomputed the intersections of HTM triangles and geographic regions for faster false-positive filtering. With full control over the index structure, HTM-based pre-filtering of simple containment searches outperforms SQL Server spa-tial indices by a factor of ten and HTM-based spatial joins run about a hundred times faster.},
address = {Aalborg, Denmark},
annote = {The paper demonstrates the capabilities of using HTM to indexing and thus align geographic point data with polygons. The authors extend the original HTM implementation with the capability to index regions/polygons. Regions are indexed through multi-level tessellation with HTM trixels. The demonstrator allows the authors to perform spatial filtering and spatial joins with a speedup of orders of magnitudes compared to SQL server spatial indexes.},
author = {Kondor, D{\'{a}}niel and Dobos, L{\'{a}}szl{\'{o}} and Csabai, Istv{\'{a}}n and Bodor, Andr{\'{a}}s and Vattay, G{\'{a}}bor and Budav{\'{a}}ri, Tam{\'{a}}s and Szalay, Alexander S},
booktitle = {Proceedings of the 26th International Conference on Scientific and Statistical Database Management},
doi = {abs/1410.0709},
editor = {Jensen, Christian S. and Lu, Hua and Pedersen, Torben Bach and Thomsen, Christian and Torp, Kristian},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kondor et al. - 2014 - Efficient classification of billions of points into complex geographic regions using hierarchical triangular m(2).pdf:pdf},
keywords = {global,global index},
title = {{Efficient classification of billions of points into complex geographic regions using hierarchical triangular mesh}},
url = {http://www.vo.elte.hu/htmpaper/ssdbm2014{\_}submission{\_}47{\_}crc.pdf http://arxiv.org/abs/1410.0709},
year = {2014}
}
@misc{Rilee2018,
author = {Rilee, Michael Lee and Kuo, Kwo-sen and Frew, James and Gallagher, James and Currey, Jon C and Walter, Jeff},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rilee et al. - 2018 - SpatioTemporal Adaptive-Resolution Encoding to Unify Diverse Earth Science Data for Integrative Analysis.pdf:pdf},
keywords = {envi,environmental,environmental informatics,global,global index},
title = {{SpatioTemporal Adaptive-Resolution Encoding to Unify Diverse Earth Science Data for Integrative Analysis}},
year = {2018}
}
@inproceedings{Planthaber2012,
abstract = {Earth scientists are increasingly experiencing difficulties with analyzing rapidly growing volumes of complex data. Those who must perform analysis directly on low-level National Aeronautics and Space Administration (NASA) Moderate Resolution Imaging Spectroradiometer (MODIS) Level 1B calibrated and geolocated data, for example, encounter an arcane, high-volume data set that is burdensome to make use of. Instead, Earth scientists typically opt to use higher-level "canned" products provided by NASA. However, when these higher-level products fail to meet the requirements of a particular project, a cruel dilemma arises: cope with data products that don't exactly meet the project's needs or spend an enormous amount of resources extracting what is needed from the unadulterated low-level data. In this paper, we present EarthDB, a system that eliminates this dilemma by offering the following contributions: 1. Enabling painless importing of MODIS Level 1B data into SciDB, a highly scalable science-oriented database platform that abstracts away the complexity of distributed storage and analysis of complex multi-dimensional data, 2. Defining a schema that unifies storage and representation of MODIS Level 1B data, regardless of its source file, 3. Supporting fast filtering and analysis of MODIS data through the use of an intuitive, high-level query language rather than complex procedural programming and, 4. Providing the ability to easily define and reconfigure entire analysis pipelines within the SciDB database, allowing for rapid ad-hoc analysis. To demonstrate this ability, we provide sample benchmarks for the construction of true-color (RGB) and Normalized Difference Vegetative Index (NDVI) images from raw MODIS Level 1B data using relatively simple queries with scalable performance.},
address = {Redondo Beach, California},
author = {Planthaber, Gary and Stonebraker, Michael and Frew, James},
booktitle = {1st ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
doi = {10.1145/2447481.2447483},
file = {:home/griessbaum/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Planthaber, Stonebraker, Frew - 2012 - EarthDB Scalable Analysis of MODIS Data using SciDB.pdf:pdf},
keywords = {Analytics,Big Data,Database,Distributed,Earth Science,EarthDB,GIS,MODIS,NASA,Performance Keywords EarthDB,Remote Sensing,SciDB,analytics,big data,database,distributed,earth science,environmental informatics,global,global index,remote sensing},
mendeley-tags = {EarthDB,GIS,MODIS,NASA,SciDB,analytics,big data,database,distributed,earth science,remote sensing},
pages = {11--19},
publisher = {ACM},
title = {{EarthDB: Scalable Analysis of MODIS Data using SciDB}},
url = {https://dl.acm.org/citation.cfm?id=2447483 http://delivery.acm.org/10.1145/2450000/2447483/p11-planthaber.pdf?ip=128.111.110.169{\&}id=2447483{\&}acc=ACTIVE SERVICE{\&}key=CA367851C7E3CE77.022A0CC51A76093F.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1529564323{\_}1cbeb},
year = {2012}
}
