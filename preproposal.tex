\documentclass[a4paper,10pt]{article}

\input{../../latex/preamble.tex}
\input{../../latex/acronyms.tex}


\usepackage[
  backend=biber,
  style=reading,
  citestyle=authoryear,
  abstract=false,
  file=false,
  entryhead=true,
  entrykey=true,
  annotation=true,
  library=true,
  loadfiles=true,
  natbib=true  
]{biblatex}




\usepackage{tipa}

\addbibresource{../../library/snow.bib} 
\addbibresource{../../library/phd.bib} 
\addbibresource{../../library/scidb.bib} 
\addbibresource{../../library/data_citation.bib} 
\addbibresource{../../library/CS274.bib} 
\addbibresource{../../library/CS270.bib} 

\title{PhD dissertation pre-proposal}
\author{Niklas Griessbaum}
\date{\today}


\begin{document}
\maketitle

\tableofcontents

\newpage
\printglossaries


\newpage

\section{Motivation}

Environmental informatics is the application of information science to environmental sciences \citep{Frew2012}.
As such, it addresses the information infrastructure that environmental scientists leverage
to obtain knowledge form environmental data.

The appearance of cloud computing, characterized by scalability on one side, 
and by abstraction and stereotyping of interactions through services and \glspl{API} on the other side \citep{Foster2017}, 
provides for opportunities in environmental informatics. 
At the same time, it opens up questions of what the workflow
of environmental scientists should look like to fully exploit these opportunities.

My thesis is that the inherent heterogeneity of the flow from data to knowledge
in the environmental sciences causes bottlenecks that can be unblocked through 
infrastructures leveraging cloud computing. 
In my dissertation, I want to address the twilight of file-centricity\footnote{German: Dateid√§mmerung \textipa{[\textsubring{d}a'ta\textsubcircum{i}'dEm@\;RUN]}} and technologies required to transition to data-centricity.

Files package (i.e. chunk or aggregate) data into logical units and provide an intelligible identity to their content: Their filename\footnote{Using the conventions of \gls{MODIS} filename as data identity was e.g. exploited by MODster \citep{Frew2005, Frew2002} to managed distributed data}.
Files further conceal the structure of the data, which allows repositories to preserve and distribute data structure-agnostically. 
While this generalizes and simplifies the task for the repositories, it pushes the responsibility of acknowledging the data's structure to the user,
who have to \gls{ETL} data from files prior to extracting knowledge \citep{Rilee2016, Szalay2009}.

The tragedy in this approach materializes in two bottlenecks: Data movement and data alignment.

\paragraph{Data Movement:}
A system that is agnostic of the structure of the data it holds is incapable of performing computations on the data. 
It can merely act as a point of preservation and distribution. Data therefore has to be moved (actually: copied) to the point of computation (Gray's 3rd Law, \cite{Szalay2009}).

Data movement however is undesired since it results in uncoordinated and unstructured duplication of data and therefore in storage waste.
Further, we are facing an increasing disparity between network speeds and compute power\footnote{User bandwidth speeds have been following Nielsen's Law \citep{Nielsen1998} and grew annually by \SI{50}{\percent} over the last 36 years while compute power has been following Moore's law \citep{Moore1975} and grew annually by \SI{60}{\percent} for the last 40 years. Source:\url{https://ourworldindata.org/grapher/transistors-per-microprocessor}}, making it less and less attractive and ultimately infeasible to move data to the point of computation \citep{Hey2009}, especially as data volumes grow: While researchers may choose to copy gigabytes worth of data for their analysis, copying petabytes will not be an option in the near future \citep{Szalay2006}.

The pre-defined package size of files hereby make matters worse: If the package size does not exactly equal the area of
interest for an analysis \citep{Gray2002}\footnote{A file might for example contain bands, areas, or time periods not needed for the analysis}, a
transfer overhead will arise.

\paragraph{Data Alignment:}
Since files allow repositories to be agnostic of data structures, it removes the repositories from the obligation to address data alignment.
Consequently, it becomes the task of the data user to align data during the \gls{ETL} process if various datasets are to be integrated.
Data alignment is to be understood as providing a common method to express (e.g. spatiotemporal) coincidence throughout all used datasets.
It can be achieved through (re-)projection, aggregation, and gridding, or indexing.
Because methods for alignment are not provided from a central point (i.e. the repository), every user has to manually align data, resulting in redundant effort.

\vspace{1cm}

In order to break the stated bottlenecks, the file centric approach has to be replaced by a system that:
\begin{itemize}
    \item is capable of providing identity to data independently of the packaging.
    \item does not require manual data alignment.
    \item voids the necessity to transfer data by being aware of the data structure and therefore being capable of performing computations at the place of storage.
\end{itemize}


\newpage

\section{Work Plan}
I want to address the following challenges to spur the twilight of file-centric workflows:

\paragraph{Provision of data identity (Data Citations):}
By abandoning files as the package of data, a natural identity of data is lost. Identity, however, is needed to refer to, and reason about data. Regardless of previous subsetting and processing, data needs to be identifiable through citations. In my dissertation, I want to develop methods to provide identity and simplify the creation of citations to data stored in online repositories.

\paragraph{Co-location and Co-alignment of data:}
To avoid data movement and redundant \gls{ETL} processes, all required data have to be readily co-located at the place of computation. In practice, this means storing data in a database providing context and schema.
The integration of heterogeneous data requires data to be aligned prior to analysis. I.e. a common concept to address spatiotemporal coincidence needs to be established throughout all the data. The \gls{STARE}\footnote{STARE is a global indexing schema that is built upon the quadtree \gls{HTM}. A review of quadtrees in geospatial indexing is provided in section \ref{lit_index}.} provides such a common concept and SciDB\footnote{\url{https://www.paradigm4.com/}} is a candidate database capable of handling large volumes of \gls{STARE}-indexed data.
In my dissertation, I want to contribute to the \gls{STARE} prototype maturing and testing by:

\begin{itemize}
 \item elaborating on pixel indexing strategies
 \item exploring the operation of \gls{STARE}-SciDB in queue-driven \gls{HPC} environments
 \item abstracting of \gls{STARE}-SciDB in higher programming languages.
\end{itemize}


\paragraph{Use Case (Multi-sensor snow mapping):}
One of the five laws postulated by Jim Gray is that a database designed for a given discipline has to be capable to answer the 20 key questions the scientists may have \citep{Hey2009, Szalay2009}.
Following this spirit, I want to verify the aptness of \gls{STARE} flavored SciDB for earth science analysis by using it for multi-sensor snow mapping.

\newpage

\subsection{Provision of data identity (Data Citations)}
Citations help to make data \acrfull{FAIR} \citep{Wilkinson2016}. In more abstract terms, data citations provide identity to data, which allows referencing and de-referencing.

Provision of identity is a key challenge in the twilight of file-centric data access, since a natural addressable identity of data is lost as soon as files as a package of data are abandoned.

Data citations differ from citations of printed material in that the cited content (i.e. the data) may evolve over time and in that meta-information such as authorship or the provenance may vary within a continuous dataset. Further, since generally speaking infinite ways to subset data are possible, data citations cannot be statically generated. They have to be machine-actionable, both in terms of dynamic creation (as a function of time and subsetting parameters) and in terms of resolving data citations back to the cited material.

I am addressing data citation through an extension to the \gls{OPeNDAP} \citep{Gallagher2005} server reference implementation hyrax, \textit{bibdap}\footnote{\url{https://github.com/NiklasPhabian/olfs}},
and the web-service \textit{occur}\footnote{\url{https://github.com/NiklasPhabian/occur}, \url{http://occur.duckdns.org}}. 
With the development of \textit{bibdap}, I want to demonstrate how a data provision service can be augmented with the capability to automatically generate and resolve machine actionable data citations subject to the dataset's static metadata (stored in the \gls{DAS}), subsetting parameters, and the time of citation creation. Bibdap uses cryptographic hashes to ensure that the de-referenced citation equals the cited data. Bibdap is meant to be a concept demonstration that can be adapted to other \gls{OPeNDAP} servers (e.g. \gls{TDS} or pydap) and other data services such as \gls{WMS} and \gls{WCS}.

\Gls{occur} is a webservice that allows users to convert \gls{OPeNDAP} queries into formatted citation snippets, much like \url{www.crosscite.org} provides a service for \glspl{DOI}.
\gls{occur} enables a user to retrieve and resolve citations from a datasource that is natively not capable of handling citations and may be useful in a transition period in which not all data servers are equipped with data citation capabilities. \gls{occur} extracts metadata from the \gls{DAS} of the \gls{OPeNDAP} resources, but also allows queries to be decorated with \glspl{DOI} to associate external metadata to a dataset. \gls{occur} further allows users to store citations. A stored citation can later be resolved through \gls{occur}, whereby \gls{occur} will verify that the data has not changed and therefore assures fixity. By storing citations at a central location uniqueness can be negotiated: \gls{occur} will detect and merge citations to the exact same data.

The development of \textit{bibdap} and \gls{occur} is supported by the \gls{ESIP} federation as a 2018 \gls{ESIP} lab project. The work on \textit{bibdap} and \gls{occur} has been presented at the 2017 Bren PhD Symposium and the 2018 \gls{ESIP} summer meeting.


I want to continue the development of bibdap and occur to address the following aspects:

\begin{itemize} 
 \item Move bibdap's citation generation and hash handling (which is currently handled by the front-end) to hyrax' back-end for improvements of performance and architectural clarity.
 %This will include augmenting the bes command line language (bescmdln). 
 \item Add data Citation support for DAP4 (Currently, data citations are only implemented for DAP2) to bibdap.
 \item Add citation support for other data servers such as \gls{OGC}'s \gls{WMS}, \gls{WCS} into \gls{occur}.  
 \item Defining guidelines on minimal metadata requirements to be included into the \gls{DAS} for successful citation generation.
 \item Exploring possibilities for ``as-of'' data requests. In its current state, bibdap and occur will throw a warning to the client if a citation is attempted to be de-referenced and the data has changed since the generation of the citation. I want to explore and implement options to de-reference citations to data that has changed since the generation of the citation. Candidate options are caching of data requests, versioning of data, or versioning of datasets.
\end{itemize}


\newpage

\subsection{Data co-location and co-alignment}
In order to relax the necessity to move data prior to knowledge generation, all datasets required for analysis have to be readily stored at the place of computation. 
The co-located datasets further need to be co-aligned to allow interoperability \citep{Kuo2017, Rilee2016}. 
Co-alignment means that spatiotemporal coincidence can be addressed in the same way for all datasets. 
Co-alignment can be achieved through a common indexing schema. 
In a simple form, this could be a regular grid. 
A side-effect of co-alignment is that it can be exploited to improve physical data placement: If spatiotemporally coinciding data is stored in physical proximity, it will, for a lot of use-case, be possible to process the data (embarrassingly) parallel in a shared-nothing architecture \citep{Kuo2017}.

Building on top of the existing technologies SciDB and the \gls{STARE} \citep{Kuo2017}, 
I started to develop a system that co-locates and co-aligns data at the place of computation. I have presented the findings of an early stage of a system that indexes and stores \gls{MODIS}-swath data in a SciDB cluster during an oral presentation at the AGU Fall meeting 2017.

I want to continue the development of this system to address the following questions:
\begin{itemize}
%\item How does a system that leverages \gls{STARE} within SciDB compare to google earth engine and the \gls{DGGS} \citep{OpenGeospatialConsortium2017}?
%\item How should SciDB schemas be designed to store STARE-indexed data for various sensors (e.g. MODIS, VIRRS, Landsat, point measurements, ASO)?
%\item How can data from a variety of sensors be efficiently STARE-indexed and loaded into SciDB?
% \item How can SciDB chunking sizes be optimized for data of unknown sparseness?
% \item How can a file-view (e.g. \gls{HDF}) onto data stored in SciDB be realized for purposes of legacy compatibility.
 \item What are the relevant criteria to decide on the indexing method for (swath) pixels? Pixels can be indexed through tessellation or through indexing of the centroids. However, how can we quantify the loss of accuracy for the different methods, and how can we quantify the storage and compute overhead for pixels indexed through tessellation? 
 \item How can (\gls{STARE}-)SciDB be operated in a queue manager driven \gls{HPC} environment? Traditionally, SciDB is run on dedicated shared-nothing hardware. However, in \gls{HPC} environments such as UCSB's Knot cluster, hardware are shared among a wide community of users through queue managers that do not allow for resources to be reserved by a DBMS. I want to explore possibilities to execute (\gls{STARE})-SciDB queries inside such an environment.
 \item How can the interactions with \gls{STARE}-indexed data stored in SciDB be abstracted from high level programming languages, much like the anticipated capabilities of STARS (\url{https://github.com/r-spatial/stars}) and openEO \url{http://openeo.org/}?
\end{itemize}


\newpage

\subsection{Use case (Multi-sensor snow mapping)}
A compute system that co-locates and co-aligns datasets has its greatest utility for
analysis on heterogeneous data; i.e. if data of various shapes are to be integrated. An example is the calculation and comparison of sub-pixel snow cover from data captured by various sensors.

\cite{Painter2009} describe the \gls{MODSCAG} algorithm to retrieve sub-pixel snow cover data as well as grainsize and albedo estimates from MOD09GA (level \gls{L2G}\footnote{L2G is the ``Level 2G'' format, which is geolocated, and gridded into a map projection}) data. 

\gls{MODSCAG} could be employed to use \gls{MODIS} level 2 (swath) data, and data from \gls{VIIRS}, \gls{AVIRIS}, Landsat, \gls{GOES}, or Himawari 8. There appears to be an opportunity to utilize \gls{STARE}-spiced SciDB to load data from a variety of sensors and produce and compare snowmaps from these data. In addition to space-born sensors, the integration of in-situ measures from snow-tracks or snow pillows, as well as airborne measurements from \gls{AVIRIS} and the \gls{ASO} can be integrated for cross-verification.

The research question that I therefore want to answer in this context is: Can the effort to extract knowledge from these heterogeneous data sources be reduced if data is co-located in a SciDB cluster and co-aligned through \gls{STARE} prior to analysis.

As part of the project ''SpatioTemporal Adaptive-Resolution Encoding to Unify Diverse Earth Science Data for Integrative Analysis'' submitted to ``Advancing Collaborative Connection for Earth System Science'' (NNH17ZDA001N-ACCESS), I will compare the performance of \gls{MODSCAG} \citep{Painter2009} executed on \gls{STARE}-indexed \gls{HDF} files and executed on \gls{STARE}-indexed SciDB arrays.

For this exercise, I will port the \gls{MODSCAG} algorithm to work on MOD09 (swath) data through the use of \gls{STARE}. I will \gls{STARE}-index a significant collection of MOD09 data for an apt region of interest (e.g. Tuolumne Basin, Merced Basin, or San Joaquin in the eastern Sierra Nevada or and/or a Himachal Pradesh in India). For verification purpose, I will index Landsat \gls{TM} data for the same geospatial and temporal extend.

I further intend to explore advancements of \gls{MODSCAG}: Jeff Dozier is currently working on a non-linear approach to retrieve sub-pixel snow cover from \gls{AVIRIS} data. There are three major difference in the new approach:
\begin{enumerate}
 \item Instead of optimizing a combination of pre-calculated canopy-top snow endmember, the new algorithm will solve for snow parameters (grain size and impurities).
 \item Instead of selecting non-snow (rock, soil, vegetation, lake ice) endmembers from a library, the algorithm will use each pixels' summer spectrum to define the non-snow part of the mixed spectra.
 \item \gls{MODSCAG}'s spectral analysis is solely based on the shape of the spectra, not by their absolute values. This allows \gls{MODSCAG} to disregard the illumination angle and hence the terrain. This is a great advantage of \gls{MODSCAG} since it allows the algorithm to produce results in mountainous regions in which the terrain is not known at a sufficient resolution. However, by disregarding the absolute reflectance, distinguishing between dirty and fresh snow becomes blurry as both small grain sizes and dirt reduce the difference between reflectance in \gls{NIR} and \gls{SW} of snow. Instead of only using the relative shapes of the spectra, the new algorithm solves for the zenith angles of the terrain through comparison of temporarily close apparent reflectance measurements of the same area with varying viewing and solar angles. This allows to estimate the actual absolute reflectances and therefore to distinguish more accurately between dirty and fresh snow.
\end{enumerate}

As of now, the algorithm is being tested on hyperspectral \gls{AVIRIS} data, which is only sparsely available. I intend to implement the algorithm in SciDB leveraging \gls{STARE} to work with multispectral data from \gls{MODIS}, \gls{VIIRS}, \gls{GOES} and/or Himawari 8.


\newpage
\section{Literature Review}

\subsection{Data citation}
In the following, I intend to explore three questions about data citations:

\begin{enumerate}
 \item Why do we need data citations?
 \item What are data citations?
 \item How have data citation systems been implemented?
\end{enumerate}

\subsubsection{Why do we need data citations}    
\cite{Hey2009} coin the term \textit{4th paradigm} as ``using computers to gain understanding from data created and stored in our electronic data stores [..]''.
The 4th paradigm arises from an environment in which large amounts of data are collected 24/7 and made publicly accessible. Research is not anymore just driven by empirical, theoretical and computational approaches, but also by the exploration of vast amounts of data collected from instruments and simulations. In this context, data collection and assembly itself is a significant research activity \citep{Frew2012}.
%We are just at the beginning of the era of the 4th paradigm and therefore a lot of challenges are still to be solved. One grave uncertainty is how we can maintain long-term data provenance and enable reproducibility throughout the end of time \citep{Hey2009}. 

A crucial step into the 4th paradigm is to acknowledge data as first class research products. As such, they have to be persistently available, documented, citable, reusable, and possibly peer-reviewed \citep{Callaghan2012, Kratz2014}. The research community has to move from data sharing to data publishing \citep{Costello2009, Kratz2014} or, in other words, has to make data \gls{FAIR} \citep{Wilkinson2016}. 

Data citation are one of the required building blocks to achieve this goal and a commonly adaption of data citations and a uniform data access is expected to benefit the progress of science \citep{CODATA2013}. However, there does not seem to be a consensus on what data publication means \citep{Kratz2014} and how data citation mechanisms, which would sufficiently motivate throughout data publishing, are to be implemented \citep{Costello2009}.

The lack of data citation standards was already criticized more than a decade ago by \cite{AltKin07}. The authors proposed a set of standards to address the issues. However, years later, \cite{Altman2015, Tenopir2011} find that even though required by publishers, researchers still too often do not make data publicly available, nor cite the data consequently. The reasons for this are both cultural and technical: 

\cite{Lawrence2011} find that traditionally only conclusions are judged. Only little attention is given to the fitness of the used data for re-interpretation. This in turn results in low appreciation for data production and publishing. \cite{Tenopir2011} additionally point out that organizations do not sufficiently provide support for data management to their researchers. Even more so, the authors stress out that researchers may actually be motivated to purposefully withhold data in order to retain their own ability to publish findings.

On the technical side, robustness, openness, and uniformity in data publication are lacking \citep{Starr2015, Koltay2016}. Cost, not so much for the storage, but for curation efforts is another reason preventing data publication \citep{Gray2002}.
\citep{Tenopir2011} states that a major reason for data withholding is the effort required to publish data. 
The ability to receive credit for cited data may increase the motivation for researchers to publish their data \citep{Crosas2011, AltKin07}. However, there is no common agreement on the implementation of data citations, especially if subsets are to be cited or data is dynamic \citep{Kratz2014, Assante2016}. \citep{Belter2014} finds that even when used, data citation practices are inconsistent. \citep{Assante2016} illustrates the range of practices which span from exporting a formatted citation string or a generic format such as RIS or BibTex, to embedded links to the dataset, or sharing to social media. 

\citep{Silvello2017} provides an exhaustive review of the current state of data citations in terms of reasons for the necessity of data citations and on current examples of data citation implementations. Based on a meta-study, the author identified 6 main motivations for data citations: Attribution, Connection, Discovery, Sharing, Impact, Reproducability. 
Arguably, these motivations can be boiled down to Identity, Credit, and Access. 

\paragraph{Identity:} 
Citations provide an identity to data, enabling to refer to and reason about data \citep{Bandrowski2016}, even if the data does no longer exist or is behind not open access.

\paragraph{Credit:}
Citations provide a way to give credit to the authors. This allows to attribute how the data was used over time, how relevant, and how impactful it is \citep{Honor2016}. The possibility to receive credit consequently provides an incentive for sharing \citep{Niemeyer2016, Callaghan2012, Kratz2014}. Initiatives such as ``Making Data Count''\citep{Kratz2015} recognize this and supply the community with principles and implementations to obtain data usage metrics.

\paragraph{Access:} 
A citation provides information on how to retrieve the cited material (e.g. the journal, year and pages). Persistent access to data is crucial since it is the foundation of reusability and reproducability \citep{Starr2015}.
%Data citations may provide an additional side-effect improving data accessibility: There is evidence suggesting that well curated data will be cited favorably \citep{Belter2014}. Citations therefore may be a motivating driver for researchers to sustainably curate their data and thus making them more accessible.


\subsubsection{What is a data citation?}
Data citation establish a link between published research results and data \citep{CODATA2013}. Minimally, a data citation consists of the following elements \citep{Cook2016}:

\begin{enumerate}
 \item Unique identity.
 \item Information about the creator.
 \item Information about how to access the data.
\end{enumerate}

Unique identities of data allow to reference and connect related work and therefore enhance discoverability. Further, they enable to evaluate the relevance and impact of the dataset. The information about the creator allows the author to claim attribution of the impact, which in turn provides incentive for the author to share the dataset. The information about how to access the data facilitates the sharing and allows reproducability.

Arguably, data citations may include a fingerprint\footnote{i.e. a hash. \citep{Crosas2011} refers to them as \gls{UNF}} to enable data integrity verification \citep{Crosas2011} and that a given dataset equals the referenced data.

Data citations further should be machine-actionable \citep{Assante2016, Altman2015, Buneman2016}, both in terms of creation and resolving and in terms of identification.
They have to accommodate the structured nature of data, the fact that data may change over time and that users may need to refer to subsets of data \citep{Buneman2010}.

Even though data citations often rely on \gls{DOI} \citep{Castelli2013}, \cite{Buneman2010} stress out that a citation is more than a \gls{DOI}.
Literature frequently appears to intermix identity with access (locator vs identifier) \citep{ESIP2012a}: While actionable identifiers such as \glspl{DOI} may provide a unique identity and an access mechanism at the same time, identity and access remain two distinct facets of a citation. There is utility in data identity regardless of whether or not the data can be accessed or even still exists.
\Cite{Parsons2013a} criticizes another aspect of \gls{DOI} use in data citations: \Glspl{DOI} are misunderstood to provide imprimaturs and persistence. However, \gls{DOI} cannot provide persistence and should solely be understood as a locator and identifier, which are required long before an imprimatur can be issued.

\newpage
\subsubsection{Solutions and implementations:}
I want to distinguish the differences in data citation implementations in five different aspects:

\begin{enumerate}
 \item How are datasets and their subsets identified? 
 \item How is fixity is assured? 
 \item How are revisable datasets handled? 
 \item How does the citation facilitate access to the data? 
 \item How are human readable citation snippets/strings generated.
\end{enumerate}

The way these aspects are solved depend widely on the domain and the particular dataset characteristics in terms of data complexity (table/arrays vs graphs), data volume (\si{\kilo\byte} vs \si{\peta\byte}), as well as update frequency and the characteristics of the repository in terms of subsetting capabilities and the typical use.

%%%%%%%%%%%%%%%%%%%%%
\paragraph{Identity:}
A unique identity could be provided by any arbitrary string, given that it is unique \citep{CODATA2013}. In some contexts, filenames \citep{Buneman2016}, or already established identifiers such as accession numbers \citep{Bandrowski2016} may serve this purpose. 
In practical terms, \cite{AltKin07} suggest that the identity should double as a handle to the data by associating it with a naming resolution service, e.g. through the use of e.g. \gls{DOI},  \gls{LSID}, \gls{URN}, \gls{HNR}\footnote{\url{http://www.handle.net/}}, or \gls{URL}. 

In contrary to traditional publications, queries may produce an infinite number of subsets from a single source \citep{Davidson2017, CODATA2013}. It is therefore not only necessary to reference a dataset, but every possible subset of a dataset. \cite{AltKin07} uses the term ``deep citation'' to describe the ability to reference subsets of data. 
Further, data may evolve over time, which opens discussion of how to identify the varying states of a datset \citep{Huber2015}.

The question arises at which granularity a unique identity, and hence a \glspl{PID} should be minted. \cite{Buneman2010} therefore introduce the concept of a ``citable unit''. A citable unit is an object of interest which could be e.g. a fact stated in a scientific paper, or a subset within a dataset. The authors argue against creating identifiers for every object of interest, but rather to wisely choose the granularity of the citable unit. A citation of an object can then be created by appending the identity of the citable unit with information about the location of the object within the citable unit. In relational databases, citable units could be defined through views \citep{Buneman2016}.

The \gls{RDA} \gls{WGDC} \citep{Rauber2015a, Rauber2015, Proll2013} suggests to identify subsets by storing (normalized) queries and associating them with a \gls{PID}. However, since methods and syntax for subsetting depend on the individual repository technology, \cite{AltKin07} suggest to cite the entire dataset and append it with a textual description of how the dataset was subsetted. The authors further suggest that if significant pre-processing is done on the dataset, it should be stored and cited with its own identity individually. 

%%%%%%%%%%%%%%%%%%%%
\paragraph{Fixity:}
Datasets may change unintentionally through data rot or malicious manipulation or intentionally, due to \gls{CUD} operations. 
Data fixity is the property of data to remain unchanged. 
Fixity checking allows to verify that data has not changed.
If included into a data citation system, fixity checking can be used to verify if a dataset contains the same data as of the time of the creation of the citation.

\cite{AltKin07, Rauber2015} suggest to include \glspl{UNF} into data citations to allow fixity checking. Though mentioned in most papers addressing data citation systems (e.g. \citep{Buneman2016, Davidson2017}), only few data citation system implementations have addressed fixity so far. In this context, \cite{Klump2016} report that the STD-DOI metadata schema rejected the inclusion of hashes as they would be dependent on the form of representation (e.g. file formats or character encoding).

%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Revisable data:}
Datasets (especially in the earth/environmental sciences) may evolve over time through updates, appendation, or deletion \citep{Klump2016}. In the following, I will refer to these datasets as revisable data and want to delimit the term from datasets changing over time due to malicious manipulation or through data rot.

Literature frequently intermixes the term ``fixity'' and the ability to cite revisable datasets. A revisable dataset is anticipated and intended to change its \textit{state} over time. A citation system consequently has be able to distinguish between the states of a revisable dataset \citep{Rauber2015, Klump2016}. However, to achieve this, merely the abstract state a citation is referencing has to remain fixed (i.e. there cannot be ambiguity of the referenced state).
This is true independently of the ability to de-reference a citation to the referenced state (i.e. the actual state being fixed). Identifying and referencing an ephemeral state of a dataset is a necessary requirement for data citation. The ability to persistently retrieve this state of the data is a data publication, not data citation challenge. It hereby is up to the publisher and repository to choose an apt level of zeal:

\begin{description}
  \item[Pessimistic] Data is assumed to be ephemeral and consequently citations cannot ever be de-referenced.
  \item[Optimistic] Data is assumed to be fixed. Citations always de-reference to the current state of the data.
  \item[Opportunistic] Data is assumed to remained fixed for some time. Citations can be de-referenced only until the data changes.
  \item[Pedantic] Every state of the data is saved. Consequently citations can always be de-referenced to the referenced state.  
\end{description}

\citep{Klump2016} makes a hard distinguishing between growing, and updated dataset. Identifying a state of a growing dataset can simply be implemented through time ranges, given that records are timestamped. 

Versioning can be used to identify states of revisable data. Note that ``version'' is sometimes used synonymously with ``state'' (of a dataset or a single record). However, in the following, I will use the term ``version'' as a policy-prescribed reference to a state.

Versioning is intuitive and can trivially be implemented e.g. by appending version number suffixes to dataset names\footnote{\url{https://library.stanford.edu/research/data-management-services/data-best-practices/data-versioning}}. 
However, since versioning is merely a \textit{policy}, there is no guarantee for enforcement. Further, ``There is currently no agreed standard or recommendation among data communities as to why, how and when data should be versioned''\footnote{\url{https://www.ands.org.au/working-with-data/data-management/data-versioning}}.
%\Cite{Barkstrom2003} defines that within a dataset version, algorithms and input parameters used to produce the dataset are constant. 
The W3C\footnote{\url{https://www.w3.org/TR/dwbp/\#dataVersioning}} provides some guidance on how revisable data should be handled on the web, however, \gls{RDA}\footnote{\url{https://rd-alliance.org/data-versioning-rda-8th-plenary-bof-meeting}} considers theses standards too simple and not scalable.

A major concern in data versioning is how to reach ``consensus about when changes to a dataset should cause it to be considered a different dataset altogether rather than a new version''\footnote{\url{https://www.w3.org/TR/dwbp/\#dataVersioning}}.
From a pure identity perspective, this question is futile since every relevant state needs to be identifiable. The distinguishing between version versus state therefore is mainly connected to the nature of \glspl{PID} (and the costs associated with minting them) \citep{Klump2016} as well as the notion of hierarchical association and provenance. 

\cite{Barkstrom2003} attempts to structure the semantics of datasets versions by suggesting a 4-level tiering: Data products (Level 1) are collections of datasets (Level 2). Datasets within a data product represent homogeneous observations but may stem from different measurement sources (e.g. instruments). A dataset may have different dataset versions (Level 3). Within a dataset version, algorithms and input (e.g. calibration) parameters are fixed. Finally, a dataset version may have different datset version variants, within which all production configurations (OS, compiler, source code) are held constant.

For the WDC-RSAT data, \glspl{DOI} are assigned at the product level. The datasets in the product may be appended without issuing a new DOI. Only when data is reprocessed (new algorithm used), new DOI are assigned to the dataset \citep{Huber2015}.

\citep{AltKin07} suggest to treat different version of datasets as separate datasets and cite them independently. This suggestion is implemented by the \gls{SEDAC}\cite{Downs2013}. Every change in the dataset triggers a version increase which is associated with a new \gls{DOI}. The landing pages of newer versions reference to older versions. 

The data publishing services Zenodo, dryad, and Fighsare offer the option to mint a \gls{DOI} to represent all dataset versions (``base-DOI'') and additional \glspl{DOI} to point to specific versions\footnote{\url{http://help.zenodo.org/}} (``version-DOI''). Figshare will automatically trigger a new version when changes are made to the data. Figshares version-\glspl{DOI} are semantic and are created from the appendation of the base-DOI with $v<x>$, where $x$ is the version number. Zenodo, on the other side refers to semantic \glspl{DOI} as bad practice and mints semantically independent \glspl{DOI} for different versions and recommends to semantically link connected \glspl{DOI}.

The \gls{BCO-DMO}\footnote{\url{https://rd-alliance.org/sites/default/files/attachment/RDA_DataCitation_BCO-DMO_Chandler_Plenary.pdf}} employs more flexible rules: Changes to the data that result in different conclusions trigger a major version increment. 
Such changes include every record update and deletion as well as changes in the data schema.
A major version increment is treated as an independent entity with its own \gls{DOI} and landing page. Inserts will trigger metadata updates. A metadata update will cause a minor version increment. Minor version increments are not treated as separate entities and share the same \gls{DOI}.

As mentioned above, it is open for debate whether or not reproducability is a hard requirement for data citations of a revisable dataset. If not, resolving of citations could be deprecated altogether (pessimistic), or only allowed until the data has changed (opportunistic). The opportunistic approach could e.g. be implemented by timestamping modifications (last modification date) or through fixity checking. The \gls{RDA} \gls{WGDC} \citep{Rauber2015} elaborates on this approach: A dataset (or a subset) should be given a new identity when the data has changed since the last time the dataset (or subset) was requested. This is recommended to be implemented through the use of a normalized query store and checksums \citep{Ball2015}.

\cite{Gray2002} however advocate for all non-regeneratable data to remain available forever and  \cite{Buneman2010} suggest a dataset to remain accessible once it has been cited. In their implementation, a citation can include a version number, allowing the system to de-refrence the citation to the according state. A similar approach is implemented in the dataverse \citep{Crosas2011}, where citations optionally can contain a dataset version number. 

A simple way of keeping previous states availability is snapshotting. However, versioning and snapshotting at fixed points in time does not suit well for users of (near) real-time data  \citep{Huber2015}. Further, depending on the size and revision frequency, storing all revisions as separate exports may not be feasible \citep{Rauber2015}. To circumvent these issues \cite{AltKin07} as well as the \gls{RDA} \gls{WGDC} \citep{Rauber2015a, Rauber2015, Proll2013} recommend to store state changes on a record-level rather than on a dataset level: Every state of every record remains persistently stored and is associated with the \gls{CUD} operation that touched it. All \gls{CUD} operations are timestamped allowing to identify the validity period of every state.

This approach is implemented by \cite{Alawini2017}, who created a citation service for the eagle-iV database. Since this database itself does not version its data (only the most recent version is visible), the authors implemented an external service that versions eagle-iV data in order to provide fixity to the users. The service tracks and stores every change in the original dataset.  The authors note that this approach is viable for eagle-iV since the dataset changes very slowly.

The concept of versioning at record-level challenges the common understanding of a dataset-wide versioning and opens a debate about whether two subsets of two different dataset-level versions containing identical data should be identified as the same subset or not. 


%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Access:}
There is a common agreement, that data citations should make use of actionable \glspl{PID} as an access mechanism. E.g. \cite{AltKin07} suggest a citation to contain an identifier that can be resolved to a landing page (not to the data itself), a requirement also specified by the \gls{JDDCP} \citep{Fenner2016}.
The landing page in turn should contain a link to the data resource. The advantage is that the identifier can be resolved regardless of whether the data is behind a paywall, or does not exist anymore.

\glspl{DOI} appear the most commonly used actionable \gls{PID} in data citations, which can be explained by its maturity and the availability of a global resolving service.  \citep{Honor2016}. Alternatives are \gls{ARK}, \gls{PURL}, \gls{URL}/permalinks, or \gls{PURL} \citep{Klump2016, Starr2015}.

The question of data access is connected to reproducability and on what granularity level changing states of a revisable dataset should be stored.

% a) PID comparison
% b) Machine actionablility

There further seems to be agreement that identifiers should be resolvable to a machine-actionable representation of a landing page, implemented e.g. through Content Negotiation.

%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Format}
Data citation system should make use of metadata standards \citep{CODATA2013} and should be capable to generate human readable citation strings in order to facilitate the use of data citations \citep{Buneman2016, Rauber2015}. 

The requirement of the metadata to construct human readable strings vary between implementations but a common intersection can be found:

\begin{tabularx}{\columnwidth}{lll lll ll}
\toprule
                                                          & PID   & Creator & Title & Subj.   & Vers.   & Descr.    & UNF \\ \midrule
DataCite\footnote{\url{https://schema.datacite.org/}}     & DOI   & Yes     & Yes   & Yes     & Yes     & Yes       & No  \\
Dublin Core\footnote{\url{https://schema.datacite.org/}}  & Yes   & Yes     & Yes   & Yes     & Yes     & Yes       & No  \\ 
Mendeley\footnote{\url{https://data.mendeley.com/}}       & DOI   & Yes     & Yes   & No      & Yes     & Yes       & No  \\
Figshare\footnote{\url{https://figshare.com/}}            & DOI   & Yes     & Yes   & Yes     & Yes     & Yes       & No  \\
Dryad                                                     & DOI   & Yes     & Yes   & Yes     & Yes     & No        & No  \\
Dataverse                                                 & DOI   & Yes     & Yes   & No      & Yes     & No        & Yes \\
Zenodo\footnote{\url{https://zenodo.org/}}                & DOI   & Yes     & Yes   & No      & Yes     & No        & No  \\
WGDC\footnote{\citep{Rauber2015}}                         & Yes   & Yes     & Yes   & No      & Yes     & No        & No  \\
\bottomrule
\end{tabularx}


Additional fields: 
Publish Date, distributor, subset definition, Institution, Related Links, File type, Type of Data, Licences, Project name, Keywords, Repository Name, location (if no resolvable identity is used).

The crosscite DOI Citation formatter \footnote{\url{https://citation.crosscite.org/}} allows to generate citation strings from metadata retrieved from landing pages through content negotiation.
The citation strings are formatted subject to styles defined through the \gls{CSL}\footnote{\url{https://citationstyles.org/}}.



\subsubsection{Recommendations}
A variety of data citation recommendations have been published within the last couple of years.
Following a list of the most notable ones:

\paragraph{\acrlong{WGDC}:}
\citep{Rauber2015} are the \gls{WGDC} recommendations for data citations.

\paragraph{\acrlong{DCC}:}
\citep{Ball2015} is the data citation recommendations of the Digital Curation Center. And mainly based on the \gls{WGDC} recommendations.

\paragraph{\acrlong{ESIP}:}
\citep{ESIP2012a} are the data citation recommendations by \gls{ESIP}

\paragraph{\acrlong{COS}:}
\citep{COS2015} are the \gls{COS} data citation recommendations

\paragraph{\acrlong{JDDCP}:}
\citep{Altman2015, Rauber2015, Fenner2016, Starr2015} are the FORCE11 \gls{JDDCP}.

\paragraph{\acrlong{CODATA}:}
\citep{CODATA2013} Are the \gls{CODATA} recommendations.

%%%%%%%%%%%%%%%%%%%
\subsubsection{Implementations}

\citep{Austin2016}: MatDB\footnote{\url{http://doi.org/10.17616/R3J917}} for engineering materials 

\citep{Alawini2017}: Eagle-i ``resource discovery'' for biomedical research.

\citep{Crosas2011}: Dataverse

\citep{Buneman2016}: \gls{MODIS} and IUPHAR DB GtoPdb for drugs in clinical use. Only theoretical

\citep{Buneman2010}: IUPHAR DB GtoPdb for drugs in clinical use

\citep{Cook2016}: Citations for Oak Ridge National Laboratory Distributed Active Archive Center (ORNL DAAC).

\citep{Honor2016}: Citations for Neuroimaging (NITRC).

\citep{Proll2013}: Provides a reference implementation for \gls{RDBMS}. Use case: Large concrete dams which are used for hydroelectric power generation

\newpage
\subsection{Global indexing schemes}
\label{lit_index}
Queries are often concerned only with a portion of the whole stored data volume.
Appropriate indexing schemes minimize the data that has to be scanned \citep{Kunszt2000}.

The idea of using hierarchical data structures to represent or index geospatial data has been discussed for decades \citep{Dutton1996, Samet1988}.
Hierarchical data structures are based on recursive decomposition of an initial planar or solid and can, for example, be implemented as quadtrees \citep{Samet1988}.

The initial applications of quadtrees in the geospatial domain have mainly focused on representation of two-dimensional data in terms of visualization and image processing and were typically based on the tessellation of squares \citep{Lugo1995}.

An early example of the use of quadtrees to represent the globe three-dimensionally is \cite{Dutton1984}, who proposed the establishment of a Geodesic Elevation Model in which locations of elevation measurements are encoded/indexed in a quadtree. 
\cite{Dutton1989} suggests the use of this quadtree for general indexing of planetary data.
The quadtree is created by recursively tessellating the facets of a regular solid, in this case, an octahedron.
The triangle faces of the octahedron are broken down with the triacon breakdown. In a single step, the triacon triples the number of facets on each iteration and results in two alternating hierarchies. Every level though is fully contained within the level two steps above; two triacon breakdown steps therefore tessellate a triangle into nine smaller triangles. The address/index/code (``gemcode'') of each triangle is the concatenation of the facet numbers (one through nine) iterated through to arrive at the triangle. The author envisages a replacement of coordinates in geospatial data with geocodes, given that the community could agree on a common method to generate geocodes.

In parallel efforts \cite{Fekete1990, Fekete1990a}, and \cite{Goodchild1992} (and later also \cite{Lugo1995}) implemented the \gls{QTM} initially suggested by \cite{Dutton1984}. While \cite{Goodchild1992} used an octrahedron as initial regular solid, \cite{Fekete1990, Fekete1990a} use a icosahedron.
The resulting structures allows to geospatially index every feature object on the planet. 
In contrary to \cite{Dutton1984}, \cite{Fekete1990, Fekete1990a, Goodchild1992, Lugo1995} iteratively tessellate each triangle into four triangles, allowing them to store each tessellated triangle with two bits.
\cite{Goodchild1992} point out that the length of a trixel address (i.e. the index), which corresponds to the level/depth in the hierarchy simultaneously indexes the size (or spatial uncertainty) of the indexed object.

\cite{Dutton1996} explored the tradeoffs of the choices of the initial solid (Tetrahedron, Ocatahedron, icosahedron) and hereby empathizes the advantages of an octahedron for practical reasons: The vertices occupy cardinal points (e.g. poles) and lie in ocean areas. Additionally, edges align with cardinal lines (equator, meridians).

The idea of indexing spherical data with a quadtree was picked up again by \cite{Barret1995} and further adapted by \cite{Kunszt2000, Kunszt2001, Szalay2005}, who developed an indexing schema for the \gls{SDSS}, which would later on be implemented into the SkyServer\footnote{The SkyServer was built by Tom Barclay, Dr. Jim Gray and Dr. Alex Szaley from the TerraServer \citep{Barclay1998, Slutz1999} source code. The latter was a project to demonstrate the real-world scalability of MS SQL Server and Windows NT Server} \citep{Szalay2002, Thakar2003}.
The authors coined the term \gls{HTM}.

All nodes of the \gls{HTM} quad-tree are spherical triangles. The quad-tree is created by recursively dividing triangles into 4 new triangles by using the parent-triangles corners and the midpoints of the triangle sides as corners for the new triangles.
The name of a new node (triangle) is the concatenation of the name of the parent triangle and an index 1 through 4. Thus, node names increase in length by two bits for every level. The authors distinguish between \gls{HTM} names and the \gls{HTM} IDs, which is the 64 bit-encoded integer of the \gls{HTM} name. 
\cite{Kondor2014} use and extend the \gls{HTM} implementation to tessellate complex regions on the earth's surface.

\cite{Planthaber2012, Planthaber2012b, Krcal2015, Hausen2016, Doan2016} experimented with storing earth-observing satellite data in the array database SciDB. In their attempts, the data is indexed through integerized latitude-longitudes. \cite{Doan2016} emphasize the importance of indexes on the database performance as they govern data placement alignment and suggests \gls{HTM} as a promising approach.

\cite{Rilee2016} advanced the HTM implementation from right-justified mapping to left-justified mapping:
In a right justified mapping, trixels that are in proximity but at different levels are mapped to separate locations on the number line\footnote{trixel \textbf{S0123} has binary HTM code 1000011011 and thus HTM id 539, while trixel S01230 has the binary HTM code of 100001101100 and thus HTM id 2156. The ids are far from each other while the both trixels share the same first 4 digits in their name prefix (and thus are contained in each other)}.
Left justified mapping respects geometric containment by right-padding HTM binary codes with zeros.
The level (which in right-justified mapping is implicitly given) is specified by the last 5 bits of the 64 bit integer.
\cite{Kuo2017} extend the implementation with a temporal component and name the resulting universal geoscience data representation \gls{STARE}.
\cite{Rilee2018} propose the integration of \gls{STARE} with \gls{OPeNDAP} and to implement cloud-based science use-cases with \gls{STARE}-enabled data access.

\newpage

\subsection{Use Case (Multi-sensor snow mapping)}

\subsubsection{Motivation}
Significant portions of earth's population rely on water originating from snow-melt \citep{Barnett2005}.
Understanding and predicting snowmelt processes therefore is crucial to manage water resources, especially considering the anticipated drastic changes in snowmelt caused by globally changing climatic conditions. Modelling the physical snowmelt requires knowledge about the spatial extent of snowcover as well as its albedo.

% Floods, vegitation/ecology, feedbacks to the climate, water ressuources, hydropower
% Snowpillows, manual snowcourses, metrological surveys are traditional ways. Remote sensing provides temporal and spatial continuous methods


\subsubsection{History of snow remote mapping}
\cite{Kharlamova2015, Hall2001} introduced level-3 snow products for \gls{MODIS} at 500 m resolution (MOD10A1, \cite{Hall2016}). In its original form, the algorithm was based on \gls{NDSI} and heuristics for declaring pixels as snow covered. The algorithm produces binary pixel maps (``SNOWMAP'') of snow cover.

\begin{equation}
 NDSI6 = \frac{band \, 4 -band \, 6}{band \, 4 + band \, 6}
\end{equation}

\cite{Salomonson2006, Salomonson2004} added a fractional snow cover product staring from version 005 of MOD10A1. The new algorithm also accompanies for the fact that detectors of band 6 on \gls{MODIS} aqua are nonfunctional, requiring the definition of a new \gls{NDSI}:

\begin{equation}
 NDSI7 = \frac{band \, 4 -band \, 7}{band \, 4 + band \, 7}
\end{equation}

Fractional snow covers are calculated by assuming a linear relationship between the NDSI and the snow cover. The model is fitted with data from Landsat ETM and resulted in the following relationships:

\begin{equation}
    FRA6T = -0.01 + 1.45 * NDIS6
\end{equation}

\begin{equation}
    FRA7T = -0.64 + 1.91 * NDIS7
\end{equation}

The authors point out that improved snow cover estimates could be achieved by accounting for the (snow free) land cover of each pixel. Additionally, the authors expect improved precision through spectral endmember approach. 

Spectral unmixing allows to determine constituent materials and their proportions contributing to the spectrum of a pixel. Hence, spectral unmixing provides a method to retrieve sub-pixel detail \citep{Keshava2003}. 

Spectral mixing results from the fact that pixels of remote sensing sensors are often too large to represent a pure constituent material and rather represent a mixture of a number of constituents materials. Spectral unmixing is an approach in which the spectrum of a mixed pixel is decomposed into the spectra of the constituent materials in order to determine the proportionate contribution of each constituent material to the mixed pixel \citep{Keshava2003}.

Spectral unmixing approaches are based on the inversion of mixing models, representing the physics of hyper-spectral mixing. Mixing models may either be linear or nonlinear. Linear mixing models can be employed if the reflectance area is contiguously divided so that the incident radiation is reflected only once. Homogeneously mixed reflectance areas may cause incident radiation to be reflected by multiple substances at once. The mixing therefore has to be modeled nonlinear \citep{Keshava2003}.
In order to solve for the fractional surface types, the error of the endmember combination is minimized.

In the context of snow, the assumption of linear mixing is valid as long as only minimal interactions between different surfaces can be assumed, such as on planar areas and/or snow cover above the tree line \citep{Painter2009}. If multiple scatterings are apparent (e.g. reflections from vegetation to snow), the mixing has to be modelled nonlinear.

\cite{Vikhamar2003} present an approach for fractional snow cover identification (SnowFrac) based on constrained linear spectral unmixing. The algorithm particularly tackles the challenge of identifying fractional snow cover in tree covered areas. It uses land-cover data to a-priori determine the non-snow endmember observable in a scene.

\citep{Painter2003} describes \gls{MEMSCAG}, a method derived from \gls{MESMA} \citep{Roberts1998}, to obtain the subpixel snowcover, grainsize and albedo for \gls{AVIRIS} pixels. The underlying method is linear spectral unmixing. Simultaneously, snow grain size and the fractional snow cover is estimated. Endmembers of pure surface covers for varying grain sizes were modeled with Mie theory. Additionally, 60 endmembers for rock, soil, vegetation, and ice are used.

\Cite{Painter2009} describes \gls{MODSCAG}, an extension of \gls{MEMSCAG} to enable subpixel snowcover, grainsize and albedo estimations for \gls{MODIS} pixels. \gls{MODSCAG} uses the shape of the spectras rather than absolute reflectances, which makes it suitable to estimate snow parameters for rough mountainous regions in which terrain and hence illumination angles are not precisely determinable. \gls{MODSCAG} accounts for nonlinear mixture, however avoids nonlinear solving through the use of pre-calculated canopy-level endmembers that are linearly combined.

A general challenge in working with \gls{MODIS} pixels that \gls{MODSCAG} is facing is the wide viewing angles of \gls{MODIS} \citep{Dozier2009, Dozier2008, Liu2008}, resulting in the following problems:
\begin{itemize}
 \item Pixels far off NADIR cover more than 10 times the area of pixels close to NADIR, causing them to overlap and therefore blur the image.
 \item Considering that the topography and vegetation is not taken into account, the viewing zenith angle/geometry is unknown. However, e.g. trees will block the view to the ground increasingly with increasing viewing angles.
To overcome these and other issues, \cite{Dozier2008} developed a space-time interpolation to recover the day's best estimates by avoiding measurements far from NADIR.
\end{itemize}





%\cite{Dozier1981} presents a method to measure surface radiant temperatures at sub-pixel
%resolution. It exemplifies a method for TIROS-N satellites.
%The fundamental principle is that a sub-pixel area with higher temperature will contribute
%proportinally more to the signal in the shorter wavelengths.
%The method assumes that only two temperature fields exist within the pixel.



%\cite{Lundquist2018} presents a method to separate snow from forest temperatures and to determine fractional snow cover from MODIS data.





% No intext Citation

% \subsection{Environment Informatics}
\nocite{Frew2004} 


% \subsection{Indexing}
\nocite{Samet1990} 
\nocite{Goodchild2002} 
\nocite{OpenGeospatialConsortium2017} 

%\subsection{Databases (274) and Operating Systems (270)}
\nocite{Adya2000} 
\nocite{Agrawal1993} 
\nocite{Baker2011} 
\nocite{Berenson1995}
\nocite{Bernstein1987} 
\nocite{Chang2008} 
\nocite{Cooper2008} 
\nocite{Das2010} 
\nocite{Das2011} 
\nocite{Das2013} 
\nocite{Decandia2007}
\nocite{DivyakantAgrawal2012}
\nocite{Elhardt1984} 
\nocite{Elmore2011} 
\nocite{Gray2006} 
\nocite{Hellerstein2007} 
\nocite{Kung1981} 
\nocite{Larson2011}
\nocite{Weikum2002} 


%\subsection OS
\nocite{McKusick1984} 
\nocite{Dijkstra2001}
\nocite{Hansen1970}
\nocite{McKusick1984}
\nocite{Meyer1988}
\nocite{Rosenblum1992}
\nocite{Sandberg1985}
\nocite{Silberschatz2005} 







\newpage
\appendix
\printbibliography[keyword={snow},title={Remote sensing of Snow}]

\newpage
\printbibliography[keyword={environmental informatics},title={Environmental Informatics}]

\newpage
\printbibliography[keyword={global index},title={Global Indexing}]

\newpage
\printbibliography[keyword={data citation},title={Data Citation}]

\newpage
\printbibliography[keyword={database},title={Database Systems}]

\newpage
\printbibliography[keyword={operating system},title={Operating Systems}]



\end{document}
